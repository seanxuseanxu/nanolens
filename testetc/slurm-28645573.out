--------------------------------------------------------------------------
A requested component was not found, or was unable to be opened.  This
means that this component is either not installed or is unable to be
used on your system (e.g., sometimes this means that shared libraries
that the component requires are unable to be found/loaded).  Note that
PMIx stopped checking at the first component that it did not find.

Host:      nid003140
Framework: psec
Component: munge
--------------------------------------------------------------------------
--------------------------------------------------------------------------
A requested component was not found, or was unable to be opened.  This
means that this component is either not installed or is unable to be
used on your system (e.g., sometimes this means that shared libraries
that the component requires are unable to be found/loaded).  Note that
PMIx stopped checking at the first component that it did not find.

Host:      nid003140
Framework: psec
Component: munge
--------------------------------------------------------------------------
slurmstepd: error:  mpi/pmix_v4: pmixp_coll_ring_reset_if_to: nid003140 [0]: pmixp_coll_ring.c:742: 0x14df30035570: collective timeout seq=0
slurmstepd: error:  mpi/pmix_v4: pmixp_coll_log: nid003140 [0]: pmixp_coll.c:286: Dumping collective state
slurmstepd: error:  mpi/pmix_v4: pmixp_coll_ring_log: nid003140 [0]: pmixp_coll_ring.c:760: 0x14df30035570: COLL_FENCE_RING state seq=0
slurmstepd: error:  mpi/pmix_v4: pmixp_coll_ring_log: nid003140 [0]: pmixp_coll_ring.c:762: my peerid: 0:nid003140
slurmstepd: error:  mpi/pmix_v4: pmixp_coll_ring_log: nid003140 [0]: pmixp_coll_ring.c:769: neighbor id: next 1:nid008305, prev 1:nid008305
slurmstepd: error:  mpi/pmix_v4: pmixp_coll_ring_log: nid003140 [0]: pmixp_coll_ring.c:779: Context ptr=0x14df300355e8, #0, in-use=0
slurmstepd: error:  mpi/pmix_v4: pmixp_coll_ring_log: nid003140 [0]: pmixp_coll_ring.c:779: Context ptr=0x14df30035620, #1, in-use=0
slurmstepd: error:  mpi/pmix_v4: pmixp_coll_ring_log: nid003140 [0]: pmixp_coll_ring.c:779: Context ptr=0x14df30035658, #2, in-use=1
slurmstepd: error:  mpi/pmix_v4: pmixp_coll_ring_log: nid003140 [0]: pmixp_coll_ring.c:790: 	 seq=0 contribs: loc=1/prev=0/fwd=1
slurmstepd: error:  mpi/pmix_v4: pmixp_coll_ring_log: nid003140 [0]: pmixp_coll_ring.c:792: 	 neighbor contribs [2]:
slurmstepd: error:  mpi/pmix_v4: pmixp_coll_ring_log: nid003140 [0]: pmixp_coll_ring.c:825: 		 done contrib: -
slurmstepd: error:  mpi/pmix_v4: pmixp_coll_ring_log: nid003140 [0]: pmixp_coll_ring.c:827: 		 wait contrib: nid008305
slurmstepd: error:  mpi/pmix_v4: pmixp_coll_ring_log: nid003140 [0]: pmixp_coll_ring.c:829: 	 status=PMIXP_COLL_RING_PROGRESS
slurmstepd: error:  mpi/pmix_v4: pmixp_coll_ring_log: nid003140 [0]: pmixp_coll_ring.c:833: 	 buf (offset/size): 208/624
slurmstepd: error:  mpi/pmix_v4: pmixp_coll_ring_reset_if_to: nid008305 [1]: pmixp_coll_ring.c:742: 0x15069c007890: collective timeout seq=0
slurmstepd: error:  mpi/pmix_v4: pmixp_coll_log: nid008305 [1]: pmixp_coll.c:286: Dumping collective state
slurmstepd: error:  mpi/pmix_v4: pmixp_coll_ring_log: nid008305 [1]: pmixp_coll_ring.c:760: 0x15069c007890: COLL_FENCE_RING state seq=0
slurmstepd: error:  mpi/pmix_v4: pmixp_coll_ring_log: nid008305 [1]: pmixp_coll_ring.c:762: my peerid: 1:nid008305
slurmstepd: error:  mpi/pmix_v4: pmixp_coll_ring_log: nid008305 [1]: pmixp_coll_ring.c:769: neighbor id: next 0:nid003140, prev 0:nid003140
slurmstepd: error:  mpi/pmix_v4: pmixp_coll_ring_log: nid008305 [1]: pmixp_coll_ring.c:779: Context ptr=0x15069c007908, #0, in-use=0
slurmstepd: error:  mpi/pmix_v4: pmixp_coll_ring_log: nid008305 [1]: pmixp_coll_ring.c:779: Context ptr=0x15069c007940, #1, in-use=0
slurmstepd: error:  mpi/pmix_v4: pmixp_coll_ring_log: nid008305 [1]: pmixp_coll_ring.c:779: Context ptr=0x15069c007978, #2, in-use=1
slurmstepd: error:  mpi/pmix_v4: pmixp_coll_ring_log: nid008305 [1]: pmixp_coll_ring.c:790: 	 seq=0 contribs: loc=0/prev=1/fwd=0
slurmstepd: error:  mpi/pmix_v4: pmixp_coll_ring_log: nid008305 [1]: pmixp_coll_ring.c:792: 	 neighbor contribs [2]:
slurmstepd: error:  mpi/pmix_v4: pmixp_coll_ring_log: nid008305 [1]: pmixp_coll_ring.c:825: 		 done contrib: nid003140
slurmstepd: error:  mpi/pmix_v4: pmixp_coll_ring_log: nid008305 [1]: pmixp_coll_ring.c:827: 		 wait contrib: -
slurmstepd: error:  mpi/pmix_v4: pmixp_coll_ring_log: nid008305 [1]: pmixp_coll_ring.c:829: 	 status=PMIXP_COLL_RING_PROGRESS
slurmstepd: error:  mpi/pmix_v4: pmixp_coll_ring_log: nid008305 [1]: pmixp_coll_ring.c:833: 	 buf (offset/size): 208/624
--------------------------------------------------------------------------
A requested component was not found, or was unable to be opened.  This
means that this component is either not installed or is unable to be
used on your system (e.g., sometimes this means that shared libraries
that the component requires are unable to be found/loaded).  Note that
PMIx stopped checking at the first component that it did not find.

Host:      nid008305
Framework: psec
Component: munge
--------------------------------------------------------------------------
--------------------------------------------------------------------------
A requested component was not found, or was unable to be opened.  This
means that this component is either not installed or is unable to be
used on your system (e.g., sometimes this means that shared libraries
that the component requires are unable to be found/loaded).  Note that
PMIx stopped checking at the first component that it did not find.

Host:      nid008305
Framework: psec
Component: munge
--------------------------------------------------------------------------
Traceback (most recent call last):
  File "/global/homes/s/seanjx/gigalens/testetc/nanolens.py", line 43, in <module>
    jax.distributed.initialize(local_device_ids=range(8))
  File "/global/homes/s/seanjx/.conda/envs/gigajax/lib/python3.12/site-packages/jax/_src/distributed.py", line 196, in initialize
    global_state.initialize(coordinator_address, num_processes, process_id,
  File "/global/homes/s/seanjx/.conda/envs/gigajax/lib/python3.12/site-packages/jax/_src/distributed.py", line 102, in initialize
    self.client.connect()
jaxlib.xla_extension.XlaRuntimeError: DEADLINE_EXCEEDED: Barrier timed out. Barrier_id: PjRT_Client_Connect. Timed out task names:
/job:jax_worker/replica:0/task:1

Additional GRPC error information from remote target unknown_target_for_coordination_leader while calling /tensorflow.CoordinationService/Barrier:
:UNKNOWN:Error received from peer  {grpc_message:"Barrier timed out. Barrier_id: PjRT_Client_Connect. Timed out task names:\n/job:jax_worker/replica:0/task:1\n", grpc_status:4, created_time:"2024-07-27T01:04:35.611852822-07:00"}
2024-07-27 01:04:36.156951: E external/tsl/tsl/distributed_runtime/coordination/coordination_service_agent.cc:493] Failed to disconnect from coordination service with status: UNAVAILABLE: failed to connect to all addresses; last error: UNKNOWN: ipv4:128.55.66.73:63685: Failed to connect to remote host: Connection refused
Additional GRPC error information from remote target unknown_target_for_coordination_leader while calling /tensorflow.CoordinationService/ShutdownTask:
:UNKNOWN:Error received from peer  {created_time:"2024-07-27T01:04:36.156891632-07:00", grpc_status:14, grpc_message:"failed to connect to all addresses; last error: UNKNOWN: ipv4:128.55.66.73:63685: Failed to connect to remote host: Connection refused"}
Proceeding with agent shutdown anyway. This is usually caused by an earlier error during execution. Check the logs (this task or the leader) for an earlier error to debug further.
slurmstepd: error:  mpi/pmix_v4: pmixp_coll_ring_reset_if_to: nid003140 [0]: pmixp_coll_ring.c:742: 0x14df30035570: collective timeout seq=1
slurmstepd: error:  mpi/pmix_v4: pmixp_coll_log: nid003140 [0]: pmixp_coll.c:286: Dumping collective state
slurmstepd: error:  mpi/pmix_v4: pmixp_coll_ring_log: nid003140 [0]: pmixp_coll_ring.c:760: 0x14df30035570: COLL_FENCE_RING state seq=1
slurmstepd: error:  mpi/pmix_v4: pmixp_coll_ring_log: nid003140 [0]: pmixp_coll_ring.c:762: my peerid: 0:nid003140
slurmstepd: error:  mpi/pmix_v4: pmixp_coll_ring_log: nid003140 [0]: pmixp_coll_ring.c:769: neighbor id: next 1:nid008305, prev 1:nid008305
slurmstepd: error:  mpi/pmix_v4: pmixp_coll_ring_log: nid003140 [0]: pmixp_coll_ring.c:779: Context ptr=0x14df300355e8, #0, in-use=0
slurmstepd: error:  mpi/pmix_v4: pmixp_coll_ring_log: nid003140 [0]: pmixp_coll_ring.c:779: Context ptr=0x14df30035620, #1, in-use=0
slurmstepd: error:  mpi/pmix_v4: pmixp_coll_ring_log: nid003140 [0]: pmixp_coll_ring.c:779: Context ptr=0x14df30035658, #2, in-use=1
slurmstepd: error:  mpi/pmix_v4: pmixp_coll_ring_log: nid003140 [0]: pmixp_coll_ring.c:790: 	 seq=1 contribs: loc=0/prev=1/fwd=0
slurmstepd: error:  mpi/pmix_v4: pmixp_coll_ring_log: nid003140 [0]: pmixp_coll_ring.c:792: 	 neighbor contribs [2]:
slurmstepd: error:  mpi/pmix_v4: pmixp_coll_ring_log: nid003140 [0]: pmixp_coll_ring.c:825: 		 done contrib: nid008305
slurmstepd: error:  mpi/pmix_v4: pmixp_coll_ring_log: nid003140 [0]: pmixp_coll_ring.c:827: 		 wait contrib: -
slurmstepd: error:  mpi/pmix_v4: pmixp_coll_ring_log: nid003140 [0]: pmixp_coll_ring.c:829: 	 status=PMIXP_COLL_RING_PROGRESS
slurmstepd: error:  mpi/pmix_v4: pmixp_coll_ring_log: nid003140 [0]: pmixp_coll_ring.c:833: 	 buf (offset/size): 187/956
slurmstepd: error:  mpi/pmix_v4: pmixp_coll_ring_reset_if_to: nid008305 [1]: pmixp_coll_ring.c:742: 0x15069c007890: collective timeout seq=1
slurmstepd: error:  mpi/pmix_v4: pmixp_coll_log: nid008305 [1]: pmixp_coll.c:286: Dumping collective state
slurmstepd: error:  mpi/pmix_v4: pmixp_coll_ring_log: nid008305 [1]: pmixp_coll_ring.c:760: 0x15069c007890: COLL_FENCE_RING state seq=1
slurmstepd: error:  mpi/pmix_v4: pmixp_coll_ring_log: nid008305 [1]: pmixp_coll_ring.c:762: my peerid: 1:nid008305
slurmstepd: error:  mpi/pmix_v4: pmixp_coll_ring_log: nid008305 [1]: pmixp_coll_ring.c:769: neighbor id: next 0:nid003140, prev 0:nid003140
slurmstepd: error:  mpi/pmix_v4: pmixp_coll_ring_log: nid008305 [1]: pmixp_coll_ring.c:779: Context ptr=0x15069c007908, #0, in-use=0
slurmstepd: error:  mpi/pmix_v4: pmixp_coll_ring_log: nid008305 [1]: pmixp_coll_ring.c:779: Context ptr=0x15069c007940, #1, in-use=0
slurmstepd: error:  mpi/pmix_v4: pmixp_coll_ring_log: nid008305 [1]: pmixp_coll_ring.c:779: Context ptr=0x15069c007978, #2, in-use=1
slurmstepd: error:  mpi/pmix_v4: pmixp_coll_ring_log: nid008305 [1]: pmixp_coll_ring.c:790: 	 seq=1 contribs: loc=1/prev=0/fwd=1
slurmstepd: error:  mpi/pmix_v4: pmixp_coll_ring_log: nid008305 [1]: pmixp_coll_ring.c:792: 	 neighbor contribs [2]:
slurmstepd: error:  mpi/pmix_v4: pmixp_coll_ring_log: nid008305 [1]: pmixp_coll_ring.c:825: 		 done contrib: -
slurmstepd: error:  mpi/pmix_v4: pmixp_coll_ring_log: nid008305 [1]: pmixp_coll_ring.c:827: 		 wait contrib: nid003140
slurmstepd: error:  mpi/pmix_v4: pmixp_coll_ring_log: nid008305 [1]: pmixp_coll_ring.c:829: 	 status=PMIXP_COLL_RING_PROGRESS
slurmstepd: error:  mpi/pmix_v4: pmixp_coll_ring_log: nid008305 [1]: pmixp_coll_ring.c:833: 	 buf (offset/size): 187/956
srun: error: nid003140: task 0: Exited with exit code 1
srun: Terminating StepId=28645573.0
slurmstepd: error:  mpi/pmix_v4: _errhandler: nid008305 [1]: pmixp_client_v2.c:212: Error handler invoked: status = -61, source = [slurm.pmix.28645573.0:1]
srun: Job step aborted: Waiting up to 32 seconds for job step to finish.
srun: error: Timed out waiting for job step to complete
